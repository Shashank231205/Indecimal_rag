# ===============================
# MODEL CONFIGURATION
# ===============================

# Embedding model (semantic retrieval)
EMBEDDING_MODEL=all-MiniLM-L6-v2

# LLM for answer generation (local, grounded)
LLM_MODEL=google/flan-t5-base

# ===============================
# RAG BEHAVIOR
# ===============================

# Number of chunks retrieved per query
TOP_K=3

# Max tokens generated by LLM
MAX_NEW_TOKENS=150

# ===============================
# SYSTEM MODE
# ===============================

# Optional: strict mode enforces refusal if context is weak
STRICT_GROUNDING=true
